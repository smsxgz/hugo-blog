1. 本周帮ldc和jh做了几个实验，精调了下SGD的学习率，确认了他们目前的方法的确可以涨点（最终eval和mixup差不多）。但是他们的方法对我而言有种莫名的撕裂感，想到了一个改进的方法，但没有想到合适的实现方法，列为下周事项。
2. 阅读了下[1]的算法和实验，他们在神经网络上的实验非常奇怪，下周尝试一下效果。
3. 阅读了minimax theorem的证明[2]，比想象中困难很多，下周三前预计读完[3]，在组会上分享一下。这篇文章是HDP书中一个定理的推广，非常有趣。如果有时间准备slide，顺便会讲一下[4]。
4. 下周开始，要多花点时间在毕业论文上。

[1] Conformal Symplectic and Relativistic Optimization
[2] ELEMENTARY PROOF FOR SION'S MINIMAX THEOREM
[3] Optimal terminal dimensionality reduction in Euclidean space
[4] Nonlinear dimension reduction via outer Bi-Lipschitz extensions
